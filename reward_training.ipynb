{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### dataset:sanduntg/test_vsort_classification\n",
        "### model: TheBloke/Llama-2-7B-Chat-GPTQ"
      ],
      "metadata": {
        "id": "KFeGDJDbK_cj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e4KMJv6K7uY"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U peft\n",
        "%pip install -U datasets\n",
        "!pip install evaluate\n",
        "!pip install -U git+https://github.com/huggingface/trl\n",
        "!pip install git+https://github.com/huggingface/accelerate\n",
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling,BitsAndBytesConfig\n",
        "from trl import RewardTrainer, SFTTrainer,RewardConfig\n",
        "from datasets import Dataset\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from peft import LoraConfig, TaskType"
      ],
      "metadata": {
        "id": "RKb-Q7inLrYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"meta-llama/Llama-2-7b-hf\" ##base model provided by meta. you need access to use this\n",
        "DATA_PATH = \"/kaggle/input/test-summarize/train.parquet\""
      ],
      "metadata": {
        "id": "KBxuvlG9L1v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(DATA_PATH)\n",
        "df = df[:10]\n",
        "raw_dataset = Dataset.from_pandas(df)\n",
        "raw_dataset"
      ],
      "metadata": {
        "id": "gy3HuJ5oL9gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")"
      ],
      "metadata": {
        "id": "Ru0ic9GCL__p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##defininig the model and tokenizer\n",
        "hf_auth = \"\"## hugginface auth token\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH,use_auth_token=hf_auth)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH,use_auth_token=hf_auth,device_map=\"auto\",quantization_config=quant_config,)"
      ],
      "metadata": {
        "id": "T73SIAZdMCxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "def formatting_func(examples):\n",
        "    kwargs = {\"padding\": \"max_length\",\n",
        "              \"truncation\": True,\n",
        "              \"max_length\": 256,\n",
        "              \"return_tensors\": \"pt\"\n",
        "              }\n",
        "\n",
        "    # Prepend the prompt and a line break to the original_response and response-1 fields.\n",
        "    prompt_plus_chosen_response = examples[\"prompt\"] + \"\\n\" + examples[\"chosen\"]\n",
        "    prompt_plus_rejected_response = examples[\"prompt\"] + \"\\n\" + examples[\"rejected\"]\n",
        "\n",
        "    # Then tokenize these modified fields.\n",
        "    tokens_chosen = tokenizer.encode_plus(prompt_plus_chosen_response, **kwargs)\n",
        "    tokens_rejected = tokenizer.encode_plus(prompt_plus_rejected_response, **kwargs)\n",
        "\n",
        "    return {\n",
        "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0], \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n",
        "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0], \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0]\n",
        "    }"
      ],
      "metadata": {
        "id": "DDhu5pS-MJM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset = raw_dataset.map(formatting_func)\n",
        "formatted_dataset = formatted_dataset.train_test_split()"
      ],
      "metadata": {
        "id": "0IKCF57iMOqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "l7621GrJMlQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        ")"
      ],
      "metadata": {
        "id": "0wzFAobFMjJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args =RewardConfig(\n",
        "        output_dir=\"rm_checkpoint/\",\n",
        "        max_length=256,\n",
        "        num_train_epochs=1,\n",
        "        logging_steps=10,\n",
        "        gradient_accumulation_steps=1,\n",
        "        save_strategy=\"steps\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=1,\n",
        "        eval_accumulation_steps=1,\n",
        "        eval_steps=500,\n",
        "        save_steps=500,\n",
        "        warmup_steps=100,\n",
        "        logging_dir=\"./logs\",\n",
        "        learning_rate=1e-5,\n",
        "        save_total_limit=1,\n",
        "        no_cuda=True\n",
        ")"
      ],
      "metadata": {
        "id": "y4cgiZpAMjyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = RewardTrainer(model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        train_dataset=formatted_dataset['train'],\n",
        "                        eval_dataset=formatted_dataset['test'],\n",
        "                         peft_config=peft_config,\n",
        "                        args= training_args,\n",
        "                        )\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "LsObNbTVMop4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}